
@inproceedings{dai_semi-supervised_2015,
	title = {Semi-supervised Sequence Learning},
	volume = {28},
	url = {https://papers.nips.cc/paper_files/paper/2015/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html},
	abstract = {We present two approaches to use unlabeled data to improve Sequence Learningwith recurrent networks. The first approach is to predict what comes next in asequence, which is a language model in {NLP}. The second approach is to use asequence autoencoder, which reads the input sequence into a vector and predictsthe input sequence again. These two algorithms can be used as a “pretraining”algorithm for a later supervised sequence learning algorithm. In other words, theparameters obtained from the pretraining step can then be used as a starting pointfor other supervised training models. In our experiments, we find that long shortterm memory recurrent networks after pretrained with the two approaches becomemore stable to train and generalize better. With pretraining, we were able toachieve strong performance in many classification tasks, such as text classificationwith {IMDB}, {DBpedia} or image recognition in {CIFAR}-10.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Dai, Andrew M and Le, Quoc V},
	urldate = {2023-06-13},
	date = {2015},
	file = {Full Text PDF:/home/zoha/Zotero/storage/U4PJFYB8/Dai and Le - 2015 - Semi-supervised Sequence Learning.pdf:application/pdf},
}

@online{noauthor_deep_nodate,
	title = {Deep Contextualized Word Representations - {ACL} Anthology},
	url = {https://aclanthology.org/N18-1202/},
	urldate = {2023-06-13},
}
