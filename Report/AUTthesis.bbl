% Generated by plain-fa.bst,  version: 0.9 (2015/05/09), for XePersian Package
% Authors: M.Amintoosi and M.Vahedi
\providecommand{\noopsort}[1]{}
\begin{thebibliography}{10}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{chen2018fast}
Chen, Yen-Chun and Bansal, Mohit.
\newblock Fast abstractive summarization with reinforce-selected sentence
  rewriting.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 675--686, 2018.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{child2019generating}
Child, Rewon, Gray, Scott, Radford, Alec, and Sutskever, Ilya.
\newblock Generating long sequences with sparse transformers, 2019.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{ELKASSAS2021113679}
El-Kassas, Wafaa~S., Salama, Cherif~R., Rafea, Ahmed~A., and Mohamed, Hoda~K.
\newblock Automatic text summarization: A comprehensive survey.
\newblock {\em Expert Systems with Applications}, 165:113679, 2021.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{elman1990finding}
Elman, Jeffrey~L.
\newblock Finding structure in time.
\newblock {\em Cognitive science}, 14(2):179--211, 1990.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{fan-etal-2018-controllable}
Fan, Angela, Grangier, David, and Auli, Michael.
\newblock Controllable abstractive summarization.
\newblock In {\em Proceedings of the 2nd Workshop on Neural Machine Translation
  and Generation}, pages 45--54, Melbourne, Australia, July 2018. Association
  for Computational Linguistics.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{Grathwohl2017BackpropagationTT}
Grathwohl, Will, Choi, Dami, Wu, Yuhuai, Roeder, Geoffrey, and Duvenaud,
  David~Kristjanson.
\newblock Backpropagation through the void: Optimizing control variates for
  black-box gradient estimation.
\newblock {\em ArXiv}, abs/1711.00123, 2017.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{hochreiter1997long}
Hochreiter, Sepp and Schmidhuber, J{\"u}rgen.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{RL_survey}
Idris, Norisma, Alomari, Ayham, Sabri, Aznul Qalid~Md, and Alsmadi, Izzat.
\newblock Deep reinforcement and transfer learning for abstractive text
  summarization: A review.
\newblock {\em Computer Speech and Language}, 71:101276, 2022.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{Ishikawa2001HybridTS}
Ishikawa, Kai, Ando, Shinichi, and Okumura, Akitoshi.
\newblock Hybrid text summarization method based on the tf method and the lead
  method.
\newblock In {\em NTCIR Conference on Evaluation of Information Access
  Technologies}, 2001.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{DeepTL_RL}
Keneshloo, Yaser, Ramakrishnan, Naren, and Reddy, Chandan~K.
\newblock Deep transfer reinforcement learning for text summarization.
\newblock {\em ArXiv}, abs/1810.06667, 2018.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{reformer}
Kitaev, Nikita, Kaiser, Lukasz, and Levskaya, Anselm.
\newblock Reformer: The efficient transformer.
\newblock In {\em International Conference on Learning Representations}, 2019.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{kryscinski-etal-2018-improving}
Kryscinski, Wojciech, Paulus, Romain, Xiong, Caiming, and Socher, Richard.
\newblock Improving abstraction in text summarization.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 1808--1817, Brussels, Belgium,
  October-November 2018. Association for Computational Linguistics.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{lee2005fuzzy}
Lee, Chang-Shing, Jian, Zhi-Wei, and Huang, Lin-Kai.
\newblock A fuzzy ontology and its application to news summarization.
\newblock {\em IEEE Transactions on Systems, Man, and Cybernetics, Part B
  (Cybernetics)}, 35(5):859--880, 2005.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{lewis-etal-2020-bart}
Lewis, Mike, Liu, Yinhan, Goyal, Naman, Ghazvininejad, Marjan, Mohamed,
  Abdelrahman, Levy, Omer, Stoyanov, Veselin, and Zettlemoyer, Luke.
\newblock {BART}: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 7871--7880, Online, July 2020. Association
  for Computational Linguistics.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{variational}
Liang, Yunlong, Meng, Fandong, Zhou, Chulun, Xu, Jinan, Chen, Yufeng, Su,
  Jinsong, and Zhou, Jie.
\newblock A variational hierarchical model for neural cross-lingual
  summarization.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 2088--2099, Dublin,
  Ireland, May 2022. Association for Computational Linguistics.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{liu2018generative}
Liu, Linqing, Lu, Yao, Yang, Min, Qu, Qiang, Zhu, Jia, and Li, Hongyan.
\newblock Generative adversarial network for abstractive text summarization.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~32, 2018.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{liu2020survey}
Liu, Qi, Kusner, Matt~J, and Blunsom, Phil.
\newblock A survey on contextual embeddings.
\newblock {\em arXiv preprint arXiv:2003.07278}, 2020.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{luhn1958automatic}
Luhn, Hans~Peter.
\newblock The automatic creation of literature abstracts.
\newblock {\em IBM Journal of research and development}, 2(2):159--165, 1958.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{Ma2022TBERTSumTT}
Ma, Tinghuai, Pan, Qian, Rong, Huan, Qian, Yurong, Tian, Yuan, and Al-Nabhan,
  Najla~Abdulrahman.
\newblock T-bertsum: Topic-aware text summarization based on bert.
\newblock {\em IEEE Transactions on Computational Social Systems}, 9:879--890,
  2022.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{GraphBased}
Malliaros, Fragkiskos~D. and Skianis, Konstantinos.
\newblock Graph-based term weighting for text categorization.
\newblock In {\em Proceedings of the 2015 IEEE/ACM International Conference on
  Advances in Social Networks Analysis and Mining 2015}, ASONAM '15, page
  1473â€“1479, New York, NY, USA, 2015. Association for Computing Machinery.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{Moratanchsurvey}
Moratanch, N. and Chitrakala, S.
\newblock A survey on abstractive text summarization.
\newblock In {\em 2016 International Conference on Circuit, Power and Computing
  Technologies (ICCPCT)}, pages 1--7, 2016.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{andhale2016overview}
Narendra, Andhale and Bewoor, Laxmi~A.
\newblock An overview of text summarization techniques.
\newblock In {\em 2016 international conference on computing communication
  control and automation (ICCUBEA)}, pages 1--7. IEEE, 2016.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{pang2023long}
Pang, Bo, Nijkamp, Erik, Kryscin?ski, Wojciech, Savarese, Silvio, Zhou, Yingbo,
  and Xiong, Caiming.
\newblock Long document summarization with top-down and bottom-up inference.
\newblock In {\em Findings of the Association for Computational Linguistics:
  EACL 2023}, pages 1237--1254, 2023.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{Parnell2022AMC}
Parnell, Jacob, Unanue, Inigo~Jauregi, and Piccardi, Massimo.
\newblock A multi-document coverage reward for relaxed multi-document
  summarization.
\newblock In {\em Annual Meeting of the Association for Computational
  Linguistics}, 2022.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{pilault2020extractive}
Pilault, Jonathan, Li, Raymond, Subramanian, Sandeep, and Pal, Christopher.
\newblock On extractive and abstractive neural document summarization with
  transformer language models.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 9308--9319, 2020.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{shapira-etal-2022-interactive}
Shapira, Ori, Pasunuru, Ramakanth, Bansal, Mohit, Dagan, Ido, and Amsterdamer,
  Yael.
\newblock Interactive query-assisted summarization via deep reinforcement
  learning.
\newblock In {\em Proceedings of the 2022 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 2551--2568, Seattle, United States, July 2022.
  Association for Computational Linguistics.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{shapira-etal-2021-extending}
Shapira, Ori, Pasunuru, Ramakanth, Ronen, Hadar, Bansal, Mohit, Amsterdamer,
  Yael, and Dagan, Ido.
\newblock Extending multi-document summarization evaluation to the interactive
  setting.
\newblock In {\em Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 657--677, Online, June 2021. Association for
  Computational Linguistics.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{sherborne2023meta}
Sherborne, Tom and Lapata, Mirella.
\newblock Meta-learning a cross-lingual manifold for semantic parsing.
\newblock {\em Transactions of the Association for Computational Linguistics},
  11:49--67, 2023.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{PoBRL}
Su, DiJia, Su, Difei, Mulvey, John~M., and Poor, H.Vincent.
\newblock Optimizing multidocument summarization by blending reinforcement
  learning policies.
\newblock {\em IEEE Transactions on Artificial Intelligence}, 4(3):416--427,
  2023.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{twostage}
Su, Ming-Hsiang, Wu, Chung-Hsien, and Cheng, Hao-Tse.
\newblock A two-stage transformer-based approach for variable-length
  abstractive summarization.
\newblock {\em IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, 28:2061--2072, 2020.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{vaswani2017attention}
Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion,
  Gomez, Aidan~N, Kaiser, Lukasz, and Polosukhin, Illia.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{Xiong2022AdaptingPT}
Xiong, Wenhan, Gupta, Anchit, Toshniwal, Shubham, Mehdad, Yashar, and tau Yih,
  Wen.
\newblock Adapting pretrained text-to-text models for long text sequences.
\newblock {\em ArXiv}, abs/2209.10052, 2022.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{yao2018dual}
Yao, Kaichun, Zhang, Libo, Du, Dawei, Luo, Tiejian, Tao, Lili, and Wu, Yanjun.
\newblock Dual encoding for abstractive text summarization.
\newblock {\em IEEE transactions on cybernetics}, 50(3):985--996, 2018.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{zaheer2020big}
Zaheer, Manzil, Guruganesh, Guru, Dubey, Kumar~Avinava, Ainslie, Joshua,
  Alberti, Chris, Ontanon, Santiago, Pham, Philip, Ravula, Anirudh, Wang,
  Qifan, Yang, Li, et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock {\em Advances in neural information processing systems},
  33:17283--17297, 2020.

\end{LTRbibitems}

\begin{LTRbibitems}
\resetlatinfont
\bibitem{zhang2020pegasus}
Zhang, Jingqing, Zhao, Yao, Saleh, Mohammad, and Liu, Peter.
\newblock Pegasus: Pre-training with extracted gap-sentences for abstractive
  summarization.
\newblock In {\em International Conference on Machine Learning}, pages
  11328--11339. PMLR, 2020.

\end{LTRbibitems}

\end{thebibliography}
