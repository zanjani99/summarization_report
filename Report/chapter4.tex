\chapter{روش های مبتنی بر یادگیری تقویتی}

کارهای تحقیقاتی در زمینه ی یادگیری تقویتی 
\LTRfootnote{reinforcement learning}
و پردازش زبان طبیعی در سال های اخیر رشد کرده است. در یادگیری تقویتی  یک عامل با محیط تعامل می کند و با آزمون و خطا، خط مشی بهینه را برای تصمیم گیری متوالی برای به حداکثر رساندن پاداش تجمعی آینده می آموزد. این پاداش می تواند یک معیار تعریف شده توسط توسعه دهنده بر اساس کار در حال حل باشد. در خلاصه سازی خودکار انتزاعی متن، نمونه‌هایی از چنین پاداش‌هایی ممکن است شامل حفظ برجستگی، مستلزم منطقی هدایت‌شده، و غیر افزونگی باشد. 



به طور کلی،  یادگیری تقویتی  در سه حوزه مختلف برای بهبود خلاصه سازی خودکار استفاده می‌شود:
\begin{enumerate}
	\item{		
		استفاده از یادگیری تقویتی به منظور حل مسائل گوناگونی که مدل‌های دنباله به دنباله عمیق قادر به حل آن‌ها نیستند، امکانات بیشتری را فراهم می‌کند. به عنوان مثال، مشکلاتی مانند کمبود نوآوری در ایجاد خلاصه‌های خلاقانه و آموزنده و کاهش کیفیت خلاصه ها در صورت افزایش طول مقالات منبع، با استفاده از سیستم‌های یادگیری تقویتی و یادگیری خط‌مشی
		\LTRfootnote{ policy learning}
		 بهبود یافته است.

 علاوه بر این  مدل‌های دنباله به دنباله عمیق را نمی‌توان برای خلاصه کردن طیف گسترده ای از اسناد استفاده کرد، زیرا مدلی که بر روی یک مجموعه داده آموزش داده می‌شود، در یک مجموعه داده دیگر به خوبی عمل نمی‌کند و قابلیت تعمیم ندارد. رویکردهای مبتنی بر یادگیری تقویتی می‌تواند این  مشکل را با استفاده ازگرادیان خط مشی انتقادی
 \LTRfootnote{self-critic policy gradient}
  و ترکیب آن با یادگیری انتقالی
\LTRfootnote{Transfer Learning (TL) }
برای انتقال دانش از یک مجموعه داده به مجموعه دیگر برطرف کنند
\cite{DeepTL_RL}.}
 \item{
  از یادگیری تقویتی برای ترکیب ویژگی های استخراجی با خلاصه انتزاعی برای استفاده از هر دو نوع خلاصه ی خودکار با الهام از رفتار انسان استفاده می‌شود. این مدل‌ها ابتدا برجسته‌ترین جملات را از سند ورودی استخراج می‌کنند، سپس با استفاده از دو شبکه: شبکه‌های استخراج‌کننده و انتزاعی، آنها را انتزاع می‌کنند. به عنوان مثال لیو و همکاران یک چارچوب متخاصم را پیشنهاد می‌کنند که مدل‌های انتزاعی و استخراجی را همزمان با استفاده از گرادیان خط ‌مشی برای بهینه‌سازی مدل انتزاعی برای خلاصه‌ای با پاداش بالا، آموزش می‌دهد که منجر به خلاصه‌ای منسجم‌تر می‌شود.[15]
}
\item{  
	
	خلاصه‌سازی اسناد، مانند سایر وظایف مولد زبان،  اغلب به دلیل استفاده از اهداف آموزشی مبتنی بر  درست‌نمایی بیشینه
	\LTRfootnote{maximum likelihood}
	 مورد انتقاد قرار گرفته است.
	درست‌نمایی بیشینه کیفیت خلاصه‌ی تولید شده را در نظر نمی‌گیرد و ممکن است خلاصه‌هایی تولید کند که فقط یک کپی از اسناد ورودی هستند، یا می‌توانند خلاصه‌هایی را بیاموزند که پر از کلمات بی‌معنی هستند. به همین دلیل، یادگیری تقویتی به عنوان جایگزینی برای بهینه‌سازی مستقیم مدل‌ها بر روی معیارهای ارزیابی و پاداش صریح به کیفیت پیش‌بینی‌های مدل استفاده شده است. 
	معیارهای مختلفی مانند روژـ۱
\LTRfootnote{ROUGE-1}
،روژ-۲
\LTRfootnote{ROUGE-2}،
روژ-‌ال
\LTRfootnote{ROUGE-L}،
امتیاز اف-۱
\LTRfootnote{F1-score}
	و امتیازبرت
	\LTRfootnote{BERTScore}
	 به عنوان پاداش در رویکردهای یادگیری تقویتی استفاده شده است. با این حال، پارنل و همکاران  استدلال می کند که استفاده از امتیازات روژ به عنوان پاداش، جنبه های مهم خلاصه سازی، مانند انتقال اطلاعات بین اسنادی در خلاصه سازی چند سندی را نادیده می گیرد و یک پاداش پوشش اصلاح شده همراه با یک برآوردگر گرادیان سیاست مبتنی بر اصول (ریلکس )
	 \LTRfootnote{modified coverage reward along with a principled policy gradient estimator (RELAX)}
	  را پیشنهاد می‌دهند
	  \cite{Parnell2022AMC}.
	   ریلکس یک برآوردگر گرادیان خط مشی است که دارای واریانس کم و بی طرفانه است. برای کارهای RL با فضاهای کنش مداوم، مانند خلاصه سازی متن، مناسب است
	   \cite{Grathwohl2017BackpropagationTT}.

	  در تابع ضرر بر حسب ریلکس \ref{eq:relax}
	  بخش اول عبارت سیاست را تشویق می کند تا خروجی هایی تولید کند که پاداش مورد انتظار بالایی دارند و بخش دوم سیاست را تشویق می کند تا خروجی هایی مشابه خروجی های تولید شده در گذشته تولید کند همچنین
	  
	  $ r $
	  نشان دهنده‌ی پاداش 
	 $  c_\phi(\tilde{z} $
	 یک متغیر کنترلی از پارامترهای $ φ $ است که انتظار می رود به شدت با پاداش کاهش واریانس همبستگی داشته باشد.
	$  p(yـs) $ 
	احتمال دنباله مشاهده شده خروجی $ y_s $ است.
	 $ z $
	  دنباله نمونه های $Gumbel-Softmax  $ است.
	 $ \tilde{z} $
	  دنباله ای از نمونه ها از یک توزیع $ Gumbel-Softmax $ مشروط بر $ y_s $ است.
	   \todo{بهترش کن}
	  
	  \begin{equation}
	  \label{eq:relax}
	  L_\text{$ RELAX $} = -[r - c_\phi(\tilde{z})]   \log p(y^{s}) \quad + c_\phi(z) - c_\phi(\tilde{z})
	  \end{equation}
	
	  
	  

	
	
  ایجاد معیارهای جدید ارزیابی براساس منابعی غیر از خلاصه های مبنایی2. معیار روژ
  \LTRfootnote{ROUGE}
   دارای سه محدودیت اصلی است: تعصب آن نسبت به شباهت واژگانی، توجه کم آن به روان بودن و خوانایی خلاصه های انتزاعی تولید شده ، و پیش نیاز سخت آن برای استفاده از خلاصه های حقیقت پایه برای تولید امتیازات. علاوه بر این، خلاصه‌های تولید شده با معیار روژ بالا معمولاً جذابیت انسانی پایینی دارند ، بنابراین، محققان معیارهای جدیدی را برای افزایش تازگی [16]، سازگاری واقعی[17] و کیفیت بر اساس
پاسخ به پرسش و رتبه‌بندی انسانی [17] با استفاده از رویکردهای پاداش‌دهی یادگیری تقویتی بدون نیاز به خلاصه‌های مبنایی ایجاد کردند.
}
\end{enumerate}


استفاده از برآورد درست‌نمایی بیشینه  در مدل های خلاصه سازی  ممکن است  با. به همین دلیل، یادگیری تقویتی به عنوان جایگزینی برای بهینه‌سازی مستقیم مدل‌ها بر روی معیارهای ارزیابی و پاداش صریح به کیفیت پیش‌بینی‌های مدل استفاده شده است. 
