@article{ELKASSAS2021113679,
	title = {Automatic text summarization: A comprehensive survey},
	journal = {Expert Systems with Applications},
	volume = {165},
	pages = {113679},
	year = {2021},
	issn = {0957-4174},
	doi = {https://doi.org/10.1016/j.eswa.2020.113679},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420305030},
	author = {Wafaa S. El-Kassas and Cherif R. Salama and Ahmed A. Rafea and Hoda K. Mohamed},
	keywords = {Automatic text summarization, Text summarization approaches, Text summarization techniques, Text summarization evaluation},
	abstract = {Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.}
}

@article{luhn1958automatic,
	title={The automatic creation of literature abstracts},
	author={Luhn, Hans Peter},
	journal={IBM Journal of research and development},
	volume={2},
	number={2},
	pages={159--165},
	year={1958},
	publisher={Ibm}
}
@article{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}
@article{elman1990finding,
	title={Finding structure in time},
	author={Elman, Jeffrey L},
	journal={Cognitive science},
	volume={14},
	number={2},
	pages={179--211},
	year={1990},
	publisher={Wiley Online Library}
}
@article{hochreiter1997long,
	title={Long short-term memory},
	author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	journal={Neural computation},
	volume={9},
	number={8},
	pages={1735--1780},
	year={1997},
	publisher={MIT press}
}

@inproceedings{zhang2020pegasus,
	title={Pegasus: Pre-training with extracted gap-sentences for abstractive summarization},
	author={Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
	booktitle={International Conference on Machine Learning},
	pages={11328--11339},
	year={2020},
	organization={PMLR}
}

@article{sherborne2023meta,
	title={Meta-learning a cross-lingual manifold for semantic parsing},
	author={Sherborne, Tom and Lapata, Mirella},
	journal={Transactions of the Association for Computational Linguistics},
	volume={11},
	pages={49--67},
	year={2023},
	publisher={MIT Press}
}

@inproceedings{andhale2016overview,
	title={An overview of text summarization techniques},
	author={Andhale, Narendra and Bewoor, Laxmi A},
	booktitle={2016 international conference on computing communication control and automation (ICCUBEA)},
	pages={1--7},
	year={2016},
	organization={IEEE}
}
@article{lee2005fuzzy,
	title={A fuzzy ontology and its application to news summarization},
	author={Lee, Chang-Shing and Jian, Zhi-Wei and Huang, Lin-Kai},
	journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
	volume={35},
	number={5},
	pages={859--880},
	year={2005},
	publisher={IEEE}
}
@inproceedings{Ishikawa2001HybridTS,
	title={Hybrid Text Summarization Method based on the TF Method and the Lead Method},
	author={Kai Ishikawa and Shinichi Ando and Akitoshi Okumura},
	booktitle={NTCIR Conference on Evaluation of Information Access Technologies},
	year={2001},
	url={https://api.semanticscholar.org/CorpusID:1619406}
}
@inproceedings{GraphBased,
	author = {Malliaros, Fragkiskos D. and Skianis, Konstantinos},
	title = {Graph-Based Term Weighting for Text Categorization},
	year = {2015},
	isbn = {9781450338547},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2808797.2808872},
	doi = {10.1145/2808797.2808872},
	abstract = {Text categorization is an important task with plenty of applications, ranging from sentiment analysis to automated news classification. In this paper, we introduce a novel graph-based approach for text categorization. Contrary to the traditional Bag-of-Words model for document representation, we consider a model in which each document is represented by a graph that encodes relationships between the different terms. The importance of a term to a document is indicated using graph-theoretic node centrality criteria. The proposed weighting scheme is able to meaningfully capture the relationships between the terms that co-occur in a document, creating feature vectors that can improve the categorization task. We perform experiments in well-known document collections, applying popular classification algorithms. Our preliminary results indicate that the proposed graph-based weighting mechanism is able to outperform existing frequency-based term weighting criteria, under appropriate parameter setting.},
	booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
	pages = {1473â€“1479},
	numpages = {7},
	location = {Paris, France},
	series = {ASONAM '15}
}

@INPROCEEDINGS{Moratanchsurvey,
	
	author={Moratanch, N. and Chitrakala, S.},
	
	booktitle={2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT)}, 
	
	title={A survey on abstractive text summarization}, 
	
	year={2016},
	
	volume={},
	
	number={},
	
	pages={1-7},
	
	doi={10.1109/ICCPCT.2016.7530193}}


@article{ALOMARI,
	title = {Deep reinforcement and transfer learning for abstractive text summarization: A review},
	journal = {Computer Speech and Language},
	volume = {71},
	pages = {101276},
	year = {2022},
	issn = {0885-2308},
	doi = {https://doi.org/10.1016/j.csl.2021.101276},
	url = {https://www.sciencedirect.com/science/article/pii/S0885230821000796},
	author = {Ayham Alomari and Norisma Idris and Aznul Qalid Md Sabri and Izzat Alsmadi}
}

@inproceedings{chen2018fast,
	title={Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting},
	author={Chen, Yen-Chun and Bansal, Mohit},
	booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages={675--686},
	year={2018}
}
@article{liu2020survey,
	title={A survey on contextual embeddings},
	author={Liu, Qi and Kusner, Matt J and Blunsom, Phil},
	journal={arXiv preprint arXiv:2003.07278},
	year={2020}
}
@inproceedings{lewis-etal-2020-bart,
	title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
	author = "Lewis, Mike  and
	Liu, Yinhan  and
	Goyal, Naman  and
	Ghazvininejad, Marjan  and
	Mohamed, Abdelrahman  and
	Levy, Omer  and
	Stoyanov, Veselin  and
	Zettlemoyer, Luke",
	booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.acl-main.703",
	doi = "10.18653/v1/2020.acl-main.703",
	pages = "7871--7880",
	abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}
@article{yao2018dual,
	title={Dual encoding for abstractive text summarization},
	author={Yao, Kaichun and Zhang, Libo and Du, Dawei and Luo, Tiejian and Tao, Lili and Wu, Yanjun},
	journal={IEEE transactions on cybernetics},
	volume={50},
	number={3},
	pages={985--996},
	year={2018},
	publisher={IEEE}
}

@article{Ma2022TBERTSumTT,
	title={T-BERTSum: Topic-Aware Text Summarization Based on BERT},
	author={Tinghuai Ma and Qian Pan and Huan Rong and Yurong Qian and Yuan Tian and Najla Abdulrahman Al-Nabhan},
	journal={IEEE Transactions on Computational Social Systems},
	year={2022},
	volume={9},
	pages={879-890},
	url={https://api.semanticscholar.org/CorpusID:237976609}
}

@inproceedings{variational,
	title = "A Variational Hierarchical Model for Neural Cross-Lingual Summarization",
	author = "Liang, Yunlong  and
	Meng, Fandong  and
	Zhou, Chulun  and
	Xu, Jinan  and
	Chen, Yufeng  and
	Su, Jinsong  and
	Zhou, Jie",
	booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.acl-long.148",
	doi = "10.18653/v1/2022.acl-long.148",
	pages = "2088--2099",
	abstract = "The goal of the cross-lingual summarization (CLS) is to convert a document in one language (e.g., English) to a summary in another one (e.g., Chinese). The CLS task is essentially the combination of machine translation (MT) and monolingual summarization (MS), and thus there exists the hierarchical relationship between MT{\&}MS and CLS. Existing studies on CLS mainly focus on utilizing pipeline methods or jointly training an end-to-end model through an auxiliary MT or MS objective. However, it is very challenging for the model to directly conduct CLS as it requires both the abilities to translate and summarize. To address this issue, we propose a hierarchical model for the CLS task, based on the conditional variational auto-encoder. The hierarchical model contains two kinds of latent variables at the local and global levels, respectively. At the local level, there are two latent variables, one for translation and the other for summarization. As for the global level, there is another latent variable for cross-lingual summarization conditioned on the two local-level variables. Experiments on two language directions (English-Chinese) verify the effectiveness and superiority of the proposed approach. In addition, we show that our model is able to generate better cross-lingual summaries than comparison models in the few-shot setting.",
}

@article{DeepTL_RL,
	title={Deep Transfer Reinforcement Learning for Text Summarization},
	author={Yaser Keneshloo and Naren Ramakrishnan and Chandan K. Reddy},
	journal={ArXiv},
	year={2018},
	volume={abs/1810.06667},
	url={https://api.semanticscholar.org/CorpusID:53116124}
}
@inproceedings{Parnell2022AMC,
	title={A Multi-Document Coverage Reward for RELAXed Multi-Document Summarization},
	author={Jacob Parnell and Inigo Jauregi Unanue and Massimo Piccardi},
	booktitle={Annual Meeting of the Association for Computational Linguistics},
	year={2022},
	url={https://api.semanticscholar.org/CorpusID:247292760}
}
@article{Grathwohl2017BackpropagationTT,
	title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},
	author={Will Grathwohl and Dami Choi and Yuhuai Wu and Geoffrey Roeder and David Kristjanson Duvenaud},
	journal={ArXiv},
	year={2017},
	volume={abs/1711.00123},
	url={https://api.semanticscholar.org/CorpusID:3535369}
}

@inproceedings{reformer,
	title={Reformer: The Efficient Transformer},
	author={Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
	booktitle={International Conference on Learning Representations},
	year={2019}
}


@article{zaheer2020big,
	title={Big bird: Transformers for longer sequences},
	author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
	journal={Advances in neural information processing systems},
	volume={33},
	pages={17283--17297},
	year={2020}
}

@misc{child2019generating,
	title={Generating Long Sequences with Sparse Transformers}, 
	author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
	year={2019},
	eprint={1904.10509},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}
@inproceedings{xiao2020systematically,
	title={Systematically Exploring Redundancy Reduction in Summarizing Long Documents},
	author={Xiao, Wen and Carenini, Giuseppe},
	booktitle={Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},
	pages={516--528},
	year={2020}
}
@inproceedings{pilault2020extractive,
	title={On extractive and abstractive neural document summarization with transformer language models},
	author={Pilault, Jonathan and Li, Raymond and Subramanian, Sandeep and Pal, Christopher},
	booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	pages={9308--9319},
	year={2020}
}
@inproceedings{pang\frac{num}{den}2023long,
	title={Long Document Summarization with Top-down and Bottom-up Inference},
	author={Pang, Bo and Nijkamp, Erik and Kry{\'s}ci{\'n}ski, Wojciech and Savarese, Silvio and Zhou, Yingbo and Xiong, Caiming},
	booktitle={Findings of the Association for Computational Linguistics: EACL 2023},
	pages={1237--1254},
	year={2023}
}

@article{Xiong2022AdaptingPT,
	title={Adapting Pretrained Text-to-Text Models for Long Text Sequences},
	author={Wenhan Xiong and Anchit Gupta and Shubham Toshniwal and Yashar Mehdad and Wen-tau Yih},
	journal={ArXiv},
	year={2022},
	volume={abs/2209.10052},
	url={https://api.semanticscholar.org/CorpusID:252407634}
}

@ARTICLE{twostage,
	
	author={Su, Ming-Hsiang and Wu, Chung-Hsien and Cheng, Hao-Tse},
	
	journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
	
	title={A Two-Stage Transformer-Based Approach for Variable-Length Abstractive Summarization}, 
	
	year={2020},
	
	volume={28},
	
	number={},
	
	pages={2061-2072},
	
	doi={10.1109/TASLP.2020.3006731}}














