@article{ELKASSAS2021113679,
	title = {Automatic text summarization: A comprehensive survey},
	journal = {Expert Systems with Applications},
	volume = {165},
	pages = {113679},
	year = {2021},
	issn = {0957-4174},
	doi = {https://doi.org/10.1016/j.eswa.2020.113679},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420305030},
	author = {Wafaa S. El-Kassas and Cherif R. Salama and Ahmed A. Rafea and Hoda K. Mohamed},
	keywords = {Automatic text summarization, Text summarization approaches, Text summarization techniques, Text summarization evaluation},
	abstract = {Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.}
}

@article{luhn1958automatic,
	title={The automatic creation of literature abstracts},
	author={Luhn, Hans Peter},
	journal={IBM Journal of research and development},
	volume={2},
	number={2},
	pages={159--165},
	year={1958},
	publisher={Ibm}
}
@article{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}
@article{elman1990finding,
	title={Finding structure in time},
	author={Elman, Jeffrey L},
	journal={Cognitive science},
	volume={14},
	number={2},
	pages={179--211},
	year={1990},
	publisher={Wiley Online Library}
}
@article{hochreiter1997long,
	title={Long short-term memory},
	author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	journal={Neural computation},
	volume={9},
	number={8},
	pages={1735--1780},
	year={1997},
	publisher={MIT press}
}

@inproceedings{zhang2020pegasus,
	title={Pegasus: Pre-training with extracted gap-sentences for abstractive summarization},
	author={Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
	booktitle={International Conference on Machine Learning},
	pages={11328--11339},
	year={2020},
	organization={PMLR}
}

@article{sherborne2023meta,
	title={Meta-learning a cross-lingual manifold for semantic parsing},
	author={Sherborne, Tom and Lapata, Mirella},
	journal={Transactions of the Association for Computational Linguistics},
	volume={11},
	pages={49--67},
	year={2023},
	publisher={MIT Press}
}

@inproceedings{andhale2016overview,
	title={An overview of text summarization techniques},
	author={ Narendra, Andhale and Bewoor, Laxmi A},
	booktitle={2016 international conference on computing communication control and automation (ICCUBEA)},
	pages={1--7},
	year={2016},
	organization={IEEE}
}
@article{lee2005fuzzy,
	title={A fuzzy ontology and its application to news summarization},
	author={Lee, Chang-Shing and Jian, Zhi-Wei and Huang, Lin-Kai},
	journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
	volume={35},
	number={5},
	pages={859--880},
	year={2005},
	publisher={IEEE}
}
@inproceedings{Ishikawa2001HybridTS,
	title={Hybrid Text Summarization Method based on the TF Method and the Lead Method},
	author={Kai Ishikawa and Shinichi Ando and Akitoshi Okumura},
	booktitle={NTCIR Conference on Evaluation of Information Access Technologies},
	year={2001},
	url={https://api.semanticscholar.org/CorpusID:1619406}
}
@inproceedings{GraphBased,
	author = {Malliaros, Fragkiskos D. and Skianis, Konstantinos},
	title = {Graph-Based Term Weighting for Text Categorization},
	year = {2015},
	isbn = {9781450338547},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2808797.2808872},
	doi = {10.1145/2808797.2808872},
	abstract = {Text categorization is an important task with plenty of applications, ranging from sentiment analysis to automated news classification. In this paper, we introduce a novel graph-based approach for text categorization. Contrary to the traditional Bag-of-Words model for document representation, we consider a model in which each document is represented by a graph that encodes relationships between the different terms. The importance of a term to a document is indicated using graph-theoretic node centrality criteria. The proposed weighting scheme is able to meaningfully capture the relationships between the terms that co-occur in a document, creating feature vectors that can improve the categorization task. We perform experiments in well-known document collections, applying popular classification algorithms. Our preliminary results indicate that the proposed graph-based weighting mechanism is able to outperform existing frequency-based term weighting criteria, under appropriate parameter setting.},
	booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
	pages = {1473â€“1479},
	numpages = {7},
	location = {Paris, France},
	series = {ASONAM '15}
}

@INPROCEEDINGS{Moratanchsurvey,
	
	author={Moratanch, N. and Chitrakala, S.},
	
	booktitle={2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT)}, 
	
	title={A survey on abstractive text summarization}, 
	
	year={2016},
	
	volume={},
	
	number={},
	
	pages={1-7},
	
	doi={10.1109/ICCPCT.2016.7530193}}



@inproceedings{chen2018fast,
	title={Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting},
	author={Chen, Yen-Chun and Bansal, Mohit},
	booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages={675--686},
	year={2018}
}
@article{liu2020survey,
	title={A survey on contextual embeddings},
	author={Liu, Qi and Kusner, Matt J and Blunsom, Phil},
	journal={arXiv preprint arXiv:2003.07278},
	year={2020}
}
@inproceedings{lewis-etal-2020-bart,
	title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
	author = "Lewis, Mike  and
	Liu, Yinhan  and
	Goyal, Naman  and
	Ghazvininejad, Marjan  and
	Mohamed, Abdelrahman  and
	Levy, Omer  and
	Stoyanov, Veselin  and
	Zettlemoyer, Luke",
	booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.acl-main.703",
	doi = "10.18653/v1/2020.acl-main.703",
	pages = "7871--7880",
	abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}
@article{yao2018dual,
	title={Dual encoding for abstractive text summarization},
	author={Yao, Kaichun and Zhang, Libo and Du, Dawei and Luo, Tiejian and Tao, Lili and Wu, Yanjun},
	journal={IEEE transactions on cybernetics},
	volume={50},
	number={3},
	pages={985--996},
	year={2018},
	publisher={IEEE}
}

@article{Ma2022TBERTSumTT,
	title={T-BERTSum: Topic-Aware Text Summarization Based on BERT},
	author={Tinghuai Ma and Qian Pan and Huan Rong and Yurong Qian and Yuan Tian and Najla Abdulrahman Al-Nabhan},
	journal={IEEE Transactions on Computational Social Systems},
	year={2022},
	volume={9},
	pages={879-890},
	url={https://api.semanticscholar.org/CorpusID:237976609}
}

@inproceedings{variational,
	title = "A Variational Hierarchical Model for Neural Cross-Lingual Summarization",
	author = "Liang, Yunlong  and
	Meng, Fandong  and
	Zhou, Chulun  and
	Xu, Jinan  and
	Chen, Yufeng  and
	Su, Jinsong  and
	Zhou, Jie",
	booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.acl-long.148",
	doi = "10.18653/v1/2022.acl-long.148",
	pages = "2088--2099",
	abstract = "The goal of the cross-lingual summarization (CLS) is to convert a document in one language (e.g., English) to a summary in another one (e.g., Chinese). The CLS task is essentially the combination of machine translation (MT) and monolingual summarization (MS), and thus there exists the hierarchical relationship between MT{\&}MS and CLS. Existing studies on CLS mainly focus on utilizing pipeline methods or jointly training an end-to-end model through an auxiliary MT or MS objective. However, it is very challenging for the model to directly conduct CLS as it requires both the abilities to translate and summarize. To address this issue, we propose a hierarchical model for the CLS task, based on the conditional variational auto-encoder. The hierarchical model contains two kinds of latent variables at the local and global levels, respectively. At the local level, there are two latent variables, one for translation and the other for summarization. As for the global level, there is another latent variable for cross-lingual summarization conditioned on the two local-level variables. Experiments on two language directions (English-Chinese) verify the effectiveness and superiority of the proposed approach. In addition, we show that our model is able to generate better cross-lingual summaries than comparison models in the few-shot setting.",
}

@article{DeepTL_RL,
	title={Deep Transfer Reinforcement Learning for Text Summarization},
	author={Yaser Keneshloo and Naren Ramakrishnan and Chandan K. Reddy},
	journal={ArXiv},
	year={2018},
	volume={abs/1810.06667},
	url={https://api.semanticscholar.org/CorpusID:53116124}
}
@inproceedings{Parnell2022AMC,
	title={A Multi-Document Coverage Reward for RELAXed Multi-Document Summarization},
	author={Jacob Parnell and Inigo Jauregi Unanue and Massimo Piccardi},
	booktitle={Annual Meeting of the Association for Computational Linguistics},
	year={2022},
	url={https://api.semanticscholar.org/CorpusID:247292760}
}
@article{Grathwohl2017BackpropagationTT,
	title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},
	author={Will Grathwohl and Dami Choi and Yuhuai Wu and Geoffrey Roeder and David Kristjanson Duvenaud},
	journal={ArXiv},
	year={2017},
	volume={abs/1711.00123},
	url={https://api.semanticscholar.org/CorpusID:3535369}
}

@inproceedings{reformer,
	title={Reformer: The Efficient Transformer},
	author={Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
	booktitle={International Conference on Learning Representations},
	year={2019}
}


@article{zaheer2020big,
	title={Big bird: Transformers for longer sequences},
	author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
	journal={Advances in neural information processing systems},
	volume={33},
	pages={17283--17297},
	year={2020}
}

@misc{child2019generating,
	title={Generating Long Sequences with Sparse Transformers}, 
	author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
	year={2019},
	eprint={1904.10509},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}
@inproceedings{xiao2020systematically,
	title={Systematically Exploring Redundancy Reduction in Summarizing Long Documents},
	author={Xiao, Wen and Carenini, Giuseppe},
	booktitle={Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},
	pages={516--528},
	year={2020}
}
@inproceedings{pilault2020extractive,
	title={On extractive and abstractive neural document summarization with transformer language models},
	author={Pilault, Jonathan and Li, Raymond and Subramanian, Sandeep and Pal, Christopher},
	booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	pages={9308--9319},
	year={2020}
}


@article{Xiong2022AdaptingPT,
	title={Adapting Pretrained Text-to-Text Models for Long Text Sequences},
	author={Wenhan Xiong and Anchit Gupta and Shubham Toshniwal and Yashar Mehdad and Wen-tau Yih},
	journal={ArXiv},
	year={2022},
	volume={abs/2209.10052},
	url={https://api.semanticscholar.org/CorpusID:252407634}
}

@ARTICLE{twostage,
	
	author={Su, Ming-Hsiang and Wu, Chung-Hsien and Cheng, Hao-Tse},
	
	journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
	
	title={A Two-Stage Transformer-Based Approach for Variable-Length Abstractive Summarization}, 
	
	year={2020},
	
	volume={28},
	
	number={},
	
	pages={2061-2072},
	
	doi={10.1109/TASLP.2020.3006731}}

@article{RL_survey,
	title = {Deep reinforcement and transfer learning for abstractive text summarization: A review},
	journal = {Computer Speech and Language},
	volume = {71},
	pages = {101276},
	year = {2022},
	issn = {0885-2308},
	doi = {https://doi.org/10.1016/j.csl.2021.101276},
	url = {https://www.sciencedirect.com/science/article/pii/S0885230821000796},
	author = { Norisma Idris and Ayham Alomari and Aznul Qalid Md Sabri and Izzat Alsmadi}
}

@inproceedings{liu2018generative,
	title={Generative adversarial network for abstractive text summarization},
	author={Liu, Linqing and Lu, Yao and Yang, Min and Qu, Qiang and Zhu, Jia and Li, Hongyan},
	booktitle={Proceedings of the AAAI conference on artificial intelligence},
	volume={32},
	number={1},
	year={2018}
}
@inproceedings{pang2023long,
	title={Long Document Summarization with Top-down and Bottom-up Inference},
	author={Pang, Bo and Nijkamp, Erik and Kryscin?ski, Wojciech and Savarese, Silvio and Zhou, Yingbo and Xiong, Caiming},
	booktitle={Findings of the Association for Computational Linguistics: EACL 2023},
	pages={1237--1254},
	year={2023}
}
@inproceedings{fan-etal-2018-controllable,
	title = "Controllable Abstractive Summarization",
	author = "Fan, Angela  and
	Grangier, David  and
	Auli, Michael",
	booktitle = "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation",
	month = jul,
	year = "2018",
	address = "Melbourne, Australia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W18-2706",
	doi = "10.18653/v1/W18-2706",
	pages = "45--54",
	abstract = "Current models for document summarization disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how much of the document the user has already read. We present a neural summarization model with a simple but effective mechanism to enable users to specify these high level attributes in order to control the shape of the final summaries to better suit their needs. With user input, our system can produce high quality summaries that follow user preferences. Without user input, we set the control variables automatically {--} on the full text CNN-Dailymail dataset, we outperform state of the art abstractive systems (both in terms of F1-ROUGE1 40.38 vs. 39.53 F1-ROUGE and human evaluation.",
}
@inproceedings{shapira-etal-2021-extending,
	title = "Extending Multi-Document Summarization Evaluation to the Interactive Setting",
	author = "Shapira, Ori  and
	Pasunuru, Ramakanth  and
	Ronen, Hadar  and
	Bansal, Mohit  and
	Amsterdamer, Yael  and
	Dagan, Ido",
	booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jun,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.naacl-main.54",
	doi = "10.18653/v1/2021.naacl-main.54",
	pages = "657--677",
	abstract = "Allowing users to interact with multi-document summarizers is a promising direction towards improving and customizing summary results. Different ideas for interactive summarization have been proposed in previous work but these solutions are highly divergent and incomparable. In this paper, we develop an end-to-end evaluation framework for interactive summarization, focusing on expansion-based interaction, which considers the accumulating information along a user session. Our framework includes a procedure of collecting real user sessions, as well as evaluation measures relying on summarization standards, but adapted to reflect interaction. All of our solutions and resources are available publicly as a benchmark, allowing comparison of future developments in interactive summarization, and spurring progress in its methodological evaluation. We demonstrate the use of our framework by evaluating and comparing baseline implementations that we developed for this purpose, which will serve as part of our benchmark. Our extensive experimentation and analysis motivate the proposed evaluation framework design and support its viability.",
}

@inproceedings{shapira-etal-2022-interactive,
	title = "Interactive Query-Assisted Summarization via Deep Reinforcement Learning",
	author = "Shapira, Ori  and
	Pasunuru, Ramakanth  and
	Bansal, Mohit  and
	Dagan, Ido  and
	Amsterdamer, Yael",
	booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jul,
	year = "2022",
	address = "Seattle, United States",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.naacl-main.184",
	doi = "10.18653/v1/2022.naacl-main.184",
	pages = "2551--2568",
	abstract = "Interactive summarization is a task that facilitates user-guided exploration of information within a document set. While one would like to employ state of the art neural models to improve the quality of interactive summarization, many such technologies cannot ingest the full document set or cannot operate at sufficient speed for interactivity. To that end, we propose two novel deep reinforcement learning models for the task that address, respectively, the subtask of summarizing salient information that adheres to user queries, and the subtask of listing suggested queries to assist users throughout their exploration. In particular, our models allow encoding the interactive session state and history to refrain from redundancy. Together, these models compose a state of the art solution that addresses all of the task requirements. We compare our solution to a recent interactive summarization system, and show through an experimental study involving real users that our models are able to improve informativeness while preserving positive user experience.",
}

@ARTICLE{PoBRL,
	
	author={Su, DiJia and Su, Difei and Mulvey, John M. and Poor, H.Vincent},
	
	journal={IEEE Transactions on Artificial Intelligence}, 
	
	title={Optimizing Multidocument Summarization by Blending Reinforcement Learning Policies}, 
	
	year={2023},
	
	volume={4},
	
	number={3},
	
	pages={416-427},
	
	doi={10.1109/TAI.2022.3201807}}



@inproceedings{kryscinski-etal-2018-improving,
	title = "Improving Abstraction in Text Summarization",
	author = "Kryscinski, Wojciech  and
	Paulus, Romain  and
	Xiong, Caiming  and
	Socher, Richard",
	booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
	month = oct # "-" # nov,
	year = "2018",
	address = "Brussels, Belgium",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D18-1207",
	doi = "10.18653/v1/D18-1207",
	pages = "1808--1817",
	abstract = "Abstractive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document. However, the level of actual abstraction as measured by novel phrases that do not appear in the source document remains low in existing approaches. We propose two techniques to improve the level of abstraction of generated summaries. First, we decompose the decoder into a contextual network that retrieves relevant parts of the source document, and a pretrained language model that incorporates prior knowledge about language generation. Second, we propose a novelty metric that is optimized directly through policy learning to encourage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document.",
}










