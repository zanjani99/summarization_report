@article{ELKASSAS2021113679,
	title = {Automatic text summarization: A comprehensive survey},
	journal = {Expert Systems with Applications},
	volume = {165},
	pages = {113679},
	year = {2021},
	issn = {0957-4174},
	doi = {https://doi.org/10.1016/j.eswa.2020.113679},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420305030},
	author = {Wafaa S. El-Kassas and Cherif R. Salama and Ahmed A. Rafea and Hoda K. Mohamed},
	keywords = {Automatic text summarization, Text summarization approaches, Text summarization techniques, Text summarization evaluation},
	abstract = {Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.}
}

@article{luhn1958automatic,
	title={The automatic creation of literature abstracts},
	author={Luhn, Hans Peter},
	journal={IBM Journal of research and development},
	volume={2},
	number={2},
	pages={159--165},
	year={1958},
	publisher={Ibm}
}
@article{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}
@article{elman1990finding,
	title={Finding structure in time},
	author={Elman, Jeffrey L},
	journal={Cognitive science},
	volume={14},
	number={2},
	pages={179--211},
	year={1990},
	publisher={Wiley Online Library}
}
@article{hochreiter1997long,
	title={Long short-term memory},
	author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	journal={Neural computation},
	volume={9},
	number={8},
	pages={1735--1780},
	year={1997},
	publisher={MIT press}
}

@inproceedings{zhang2020pegasus,
	title={Pegasus: Pre-training with extracted gap-sentences for abstractive summarization},
	author={Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
	booktitle={International Conference on Machine Learning},
	pages={11328--11339},
	year={2020},
	organization={PMLR}
}

@article{sherborne2023meta,
	title={Meta-learning a cross-lingual manifold for semantic parsing},
	author={Sherborne, Tom and Lapata, Mirella},
	journal={Transactions of the Association for Computational Linguistics},
	volume={11},
	pages={49--67},
	year={2023},
	publisher={MIT Press}
}

@inproceedings{andhale2016overview,
	title={An overview of text summarization techniques},
	author={Andhale, Narendra and Bewoor, Laxmi A},
	booktitle={2016 international conference on computing communication control and automation (ICCUBEA)},
	pages={1--7},
	year={2016},
	organization={IEEE}
}
@article{lee2005fuzzy,
	title={A fuzzy ontology and its application to news summarization},
	author={Lee, Chang-Shing and Jian, Zhi-Wei and Huang, Lin-Kai},
	journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
	volume={35},
	number={5},
	pages={859--880},
	year={2005},
	publisher={IEEE}
}
@inproceedings{Ishikawa2001HybridTS,
	title={Hybrid Text Summarization Method based on the TF Method and the Lead Method},
	author={Kai Ishikawa and Shinichi Ando and Akitoshi Okumura},
	booktitle={NTCIR Conference on Evaluation of Information Access Technologies},
	year={2001},
	url={https://api.semanticscholar.org/CorpusID:1619406}
}
@inproceedings{GraphBased,
	author = {Malliaros, Fragkiskos D. and Skianis, Konstantinos},
	title = {Graph-Based Term Weighting for Text Categorization},
	year = {2015},
	isbn = {9781450338547},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2808797.2808872},
	doi = {10.1145/2808797.2808872},
	abstract = {Text categorization is an important task with plenty of applications, ranging from sentiment analysis to automated news classification. In this paper, we introduce a novel graph-based approach for text categorization. Contrary to the traditional Bag-of-Words model for document representation, we consider a model in which each document is represented by a graph that encodes relationships between the different terms. The importance of a term to a document is indicated using graph-theoretic node centrality criteria. The proposed weighting scheme is able to meaningfully capture the relationships between the terms that co-occur in a document, creating feature vectors that can improve the categorization task. We perform experiments in well-known document collections, applying popular classification algorithms. Our preliminary results indicate that the proposed graph-based weighting mechanism is able to outperform existing frequency-based term weighting criteria, under appropriate parameter setting.},
	booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
	pages = {1473â€“1479},
	numpages = {7},
	location = {Paris, France},
	series = {ASONAM '15}
}

@INPROCEEDINGS{Moratanchsurvey,
	
	author={Moratanch, N. and Chitrakala, S.},
	
	booktitle={2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT)}, 
	
	title={A survey on abstractive text summarization}, 
	
	year={2016},
	
	volume={},
	
	number={},
	
	pages={1-7},
	
	doi={10.1109/ICCPCT.2016.7530193}}


@article{ALOMARI,
	title = {Deep reinforcement and transfer learning for abstractive text summarization: A review},
	journal = {Computer Speech and Language},
	volume = {71},
	pages = {101276},
	year = {2022},
	issn = {0885-2308},
	doi = {https://doi.org/10.1016/j.csl.2021.101276},
	url = {https://www.sciencedirect.com/science/article/pii/S0885230821000796},
	author = {Ayham Alomari and Norisma Idris and Aznul Qalid Md Sabri and Izzat Alsmadi}
}


@inproceedings{variational,
	title = "A Variational Hierarchical Model for Neural Cross-Lingual Summarization",
	author = "Liang, Yunlong  and
	Meng, Fandong  and
	Zhou, Chulun  and
	Xu, Jinan  and
	Chen, Yufeng  and
	Su, Jinsong  and
	Zhou, Jie",
	booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.acl-long.148",
	doi = "10.18653/v1/2022.acl-long.148",
	pages = "2088--2099",
	abstract = "The goal of the cross-lingual summarization (CLS) is to convert a document in one language (e.g., English) to a summary in another one (e.g., Chinese). The CLS task is essentially the combination of machine translation (MT) and monolingual summarization (MS), and thus there exists the hierarchical relationship between MT{\&}MS and CLS. Existing studies on CLS mainly focus on utilizing pipeline methods or jointly training an end-to-end model through an auxiliary MT or MS objective. However, it is very challenging for the model to directly conduct CLS as it requires both the abilities to translate and summarize. To address this issue, we propose a hierarchical model for the CLS task, based on the conditional variational auto-encoder. The hierarchical model contains two kinds of latent variables at the local and global levels, respectively. At the local level, there are two latent variables, one for translation and the other for summarization. As for the global level, there is another latent variable for cross-lingual summarization conditioned on the two local-level variables. Experiments on two language directions (English-Chinese) verify the effectiveness and superiority of the proposed approach. In addition, we show that our model is able to generate better cross-lingual summaries than comparison models in the few-shot setting.",
}

@article{DeepTL_RL,
	title={Deep Transfer Reinforcement Learning for Text Summarization},
	author={Yaser Keneshloo and Naren Ramakrishnan and Chandan K. Reddy},
	journal={ArXiv},
	year={2018},
	volume={abs/1810.06667},
	url={https://api.semanticscholar.org/CorpusID:53116124}
}
@inproceedings{Parnell2022AMC,
	title={A Multi-Document Coverage Reward for RELAXed Multi-Document Summarization},
	author={Jacob Parnell and Inigo Jauregi Unanue and Massimo Piccardi},
	booktitle={Annual Meeting of the Association for Computational Linguistics},
	year={2022},
	url={https://api.semanticscholar.org/CorpusID:247292760}
}
@article{Grathwohl2017BackpropagationTT,
	title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},
	author={Will Grathwohl and Dami Choi and Yuhuai Wu and Geoffrey Roeder and David Kristjanson Duvenaud},
	journal={ArXiv},
	year={2017},
	volume={abs/1711.00123},
	url={https://api.semanticscholar.org/CorpusID:3535369}
}
























